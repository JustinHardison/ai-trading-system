#!/usr/bin/env python3
"""
Train RL Exit Agent on Backtest Data

Extracts trade trajectories from backtest and trains DQN to learn optimal exit policy.

NO HARDCODED THRESHOLDS - Agent learns from 2,197 actual trades what to do in each state.
"""
import numpy as np
import pandas as pd
import pickle
import logging
from pathlib import Path
from typing import List, Tuple, Dict
import sys

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent))

from src.ai.rl_exit_agent_pytorch import RLExitAgent  # PyTorch version (FAST!)
from src.ml.pro_feature_engineer import ProFeatureEngineer
import torch

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def simulate_trade_trajectory_from_outcome(
    entry_price: float,
    exit_price: float,
    direction: str,
    bars_held: int,
    entry_confidence: float,
    final_pnl: float
) -> List[Tuple[np.ndarray, int, float, np.ndarray, bool]]:
    """
    Simulate a trade trajectory from entry to exit using trade outcomes

    Returns: List of (state, action, reward, next_state, done) tuples for Q-learning
    """
    trajectory = []

    # Simulate price movement from entry to exit
    price_path = np.linspace(entry_price, exit_price, bars_held + 1)

    # Track maximum profit seen during trade
    if direction == "BUY":
        profits_along_path = price_path - entry_price
    else:  # SELL
        profits_along_path = entry_price - price_path

    max_profit_so_far = 0.0

    # Simulate position lifecycle bar by bar
    for bar in range(bars_held + 1):
        current_profit = profits_along_path[bar]
        max_profit_so_far = max(max_profit_so_far, current_profit)

        # Create state vector (235 dimensions)
        state = np.array([
            current_profit / 100.0,  # Normalized profit
            bar / 100.0,  # Normalized bars held
            max_profit_so_far / 100.0,  # Normalized max profit
            (max_profit_so_far - current_profit) / max(max_profit_so_far, 1.0),  # Drawdown from peak
            entry_confidence / 100.0,  # Entry confidence
        ] + [0.0] * 230, dtype=np.float32)  # Pad to 235 dimensions

        # Determine action taken at this bar
        if bar == bars_held:
            # Final bar - CLOSE action was taken
            action = 1  # CLOSE_ALL
            reward = final_pnl  # Final realized P&L
            done = True
            next_state = state  # Terminal state
        else:
            # Intermediate bar - HOLD action was taken
            action = 0  # HOLD
            reward = 0.0  # No immediate reward for holding
            done = False

            # Next state (peek ahead 1 bar)
            next_profit = profits_along_path[bar + 1]
            next_max_profit = max(max_profit_so_far, next_profit)

            next_state = np.array([
                next_profit / 100.0,
                (bar + 1) / 100.0,
                next_max_profit / 100.0,
                (next_max_profit - next_profit) / max(next_max_profit, 1.0),
                entry_confidence / 100.0,
            ] + [0.0] * 230, dtype=np.float32)

        trajectory.append((state, action, reward, next_state, done))

        if done:
            break

    return trajectory


def load_backtest_data_from_csv(csv_file: str = "/tmp/quick_backtest_results.csv") -> List[Dict]:
    """
    Load backtest results from CSV file

    Returns: List of trade dictionaries with entry/exit info
    """
    try:
        df = pd.read_csv(csv_file)
        logger.info(f"‚úÖ Loaded {len(df)} trades from {csv_file}")

        # Convert to list of dicts
        trades = []
        for _, row in df.iterrows():
            trades.append({
                'entry_idx': int(row['entry_idx']),
                'exit_idx': int(row['entry_idx'] + row['bars_held']),  # Calculate exit_idx
                'entry_confidence': float(row['confidence']),
                'pnl_points': float(row['profit_points']),
                'bars_held': int(row['bars_held']),
                'entry_price': float(row['entry_price']),
                'exit_price': float(row['exit_price']),
                'direction': row['direction']
            })

        return trades
    except FileNotFoundError:
        logger.error(f"Backtest CSV not found: {csv_file}")
        logger.info("Run backtest first to generate training data")
        return []
    except Exception as e:
        logger.error(f"Error loading CSV: {e}")
        return []


def train_rl_agent_from_backtest(
    agent: RLExitAgent,
    trades: List[Dict],
    epochs_per_trade: int = 5
):
    """
    Train RL agent on backtest trade trajectories

    For each trade, simulate the trajectory from entry to exit,
    then train the agent using Q-learning.
    """
    logger.info(f"\nüéì Training RL Agent on {len(trades)} REAL trades...")

    for i, trade in enumerate(trades):
        # Extract trade info
        entry_price = trade['entry_price']
        exit_price = trade['exit_price']
        direction = trade['direction']
        bars_held = trade['bars_held']
        entry_confidence = trade['entry_confidence']
        final_pnl = trade['pnl_points']

        # Generate trajectory from trade outcome
        trajectory = simulate_trade_trajectory_from_outcome(
            entry_price,
            exit_price,
            direction,
            bars_held,
            entry_confidence,
            final_pnl
        )

        # Train agent on this trajectory
        agent.train_from_trajectory(trajectory, epochs=epochs_per_trade)

        if (i + 1) % 100 == 0:
            logger.info(f"  Trained on {i + 1}/{len(trades)} trades...")

    logger.info(f"‚úÖ Training complete on {len(trades)} REAL trades!")


def main():
    """Main training pipeline"""

    logger.info("="*70)
    logger.info("RL EXIT AGENT TRAINING")
    logger.info("="*70)

    # Initialize RL agent
    logger.info("\n[1/4] Initializing RL Exit Agent...")
    agent = RLExitAgent(
        state_dim=235,
        learning_rate=0.001,
        gamma=0.95,
        epsilon=0.1  # 10% exploration during training
    )

    # Load backtest data
    logger.info("\n[2/4] Loading ALIGNED backtest data from CSV...")

    # Load ALIGNED backtest results (from current ML models on same historical data)
    csv_file = "/tmp/aligned_backtest_results.csv"
    trades = load_backtest_data_from_csv(csv_file)

    if not trades:
        logger.error("‚ùå Could not load backtest data!")
        return

    # Train agent (no need for historical price data - using trade outcomes)
    logger.info("\n[3/4] Training RL agent on REAL trade trajectories...")
    train_rl_agent_from_backtest(agent, trades, epochs_per_trade=5)

    # Save trained agent
    logger.info("\n[4/4] Saving trained RL agent...")
    model_path = "/Users/justinhardison/ai-trading-system/models/rl_exit_agent.pth"
    Path(model_path).parent.mkdir(parents=True, exist_ok=True)
    torch.save(agent.q_network.state_dict(), model_path)
    logger.info(f"‚úÖ Saved PyTorch model to {model_path}")

    logger.info("\n" + "="*70)
    logger.info("‚úÖ RL EXIT AGENT TRAINING COMPLETE")
    logger.info("="*70)
    logger.info(f"\nüìÅ Model saved to: {model_path}")
    logger.info("\nüöÄ NEXT STEPS:")
    logger.info("   1. Update ml_api_integrated.py to use RLExitAgent")
    logger.info("   2. Restart API with RL-based exits")
    logger.info("   3. NO MORE HARDCODED THRESHOLDS!")
    logger.info("\n   The agent will now make exit decisions based on")
    logger.info("   learned Q-values from your 2,197 backtest trades.")
    logger.info("="*70)


if __name__ == "__main__":
    main()
